"""face_detection_part.ipynb
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1j60R8t8DiRmD7kl3_m5SyR9yeYVydeDV
"""
import timm
import torchvision
import torch 
import numpy as np 
from torch import nn
from torchvision import datasets, transforms
import matplotlib.pyplot as plt
import cv2 
import PIL 
from PIL import Image
from detect_face import *

IdxToEmotion = {0: 'Neutral', 1: 'Happiness', 2: 'Sadness', 3:
    'Surprise', 4: 'Fear', 5: 'Disgust', 6: 'Anger', 7: 'Contempt'}

def load_emomodel(args):
    model = timm.create_model('tf_efficientnet_b0_ns', pretrained=False)
    model.classifier = nn.Sequential(nn.Linear(in_features=1280, out_features=8))
    model_dir = 'weights/enet_b2_8_best.pt'  # directory to pt file
    if args.device == 'cuda' :
        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
    else :
        device = torch.device('cpu')
    model = torch.load(model_dir, map_location=device)
    return model

def Get_Emotransform() :
    IMG_SIZE = 260
    train_transforms = transforms.Compose(
        [
            transforms.Resize((IMG_SIZE,IMG_SIZE)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                         std=[0.229, 0.224, 0.225])
        ]
    )
    test_transforms = transforms.Compose(
        [
            transforms.Resize((IMG_SIZE,IMG_SIZE)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                         std=[0.229, 0.224, 0.225])
        ]
    )
    return test_transforms



if __name__ == '__main__' :

    parser = argparse.ArgumentParser()
    parser.add_argument("--image_path", default='sample_data', help='directory to your image_data')

    ## arguments for FaceDetector_RetinaFace
    parser.add_argument('-m', '--trained_model', default='./weights/Resnet50_Final.pth',
                        type=str, help='Trained state_dict file path to open')
    parser.add_argument('--network', default='resnet50', help='Backbone network mobile0.25 or resnet50')
    parser.add_argument('--origin_size', default=True, type=str, help='Whether use origin image size to evaluate')
    parser.add_argument('--save_folder', default='./widerface_evaluate/widerface_txt/', type=str,
                        help='Dir to save txt results')
    parser.add_argument('--cpu', action="store_true", default=True, help='Use cpu inference')
    parser.add_argument('--dataset_folder', default='./data/widerface/val/images/', type=str, help='dataset path')
    parser.add_argument('--confidence_threshold', default=0.02, type=float, help='confidence_threshold')
    parser.add_argument('--top_k', default=5000, type=int, help='top_k')
    parser.add_argument('--nms_threshold', default=0.4, type=float, help='nms_threshold')
    parser.add_argument('--keep_top_k', default=750, type=int, help='keep_top_k')
    parser.add_argument('-s', '--save_image', action="store_true", default=True, help='show detection results')
    parser.add_argument('--vis_thres', default=0.5, type=float, help='visualization_threshold')
    parser.add_argument('--face_detector', default="Resnet")
    parser.add_argument('--device', default='cpu')

    args = parser.parse_args()

    model = load_emomodel(args)

    a = torch.rand((1,3,260,260))
    print(model(a))